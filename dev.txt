---

> Data Serialization and Evolution

When sending data over the network or storing it in a file, we need a way to encode the data into bytes
* programming language specific serialization such as Java serialization makes consuming the data in other languages inconvenient
* language agnostic formats such as XML and JSON have two main drawbacks
  - allowing fields to be arbitrarily added or removed with lack of structure may cause data corruption
  - field names and type information are redundant, verbose and add overhead reducing performances
* alternatives are Avro, Thrift and Protocol Buffers
* Avro is recommended by Confluent, which provides enterprise support for Kafka ecosystem

---

> Avro Schema

An Avro schema defines the data structure in a JSON format

User.avsc
{
  "namespace": "com.kafka.demo",
  "type": "record",
  "name": "User",
  "fields": [
    {
      "name": "name",
      "type": "string"
    },
    {
      "name": "age",
      "type": "int"
    }
  ]
}

* can be used to serialize a Java object (POJO) into bytes and deserialize these bytes back into the Java object
* it requires a schema during both data serialization and deserialization
* because the schema is provided at decoding time, metadata such as the field names don't have to be explicitly encoded in the data
* provides schema evolution globally or for subjects
  - Backward Compatibility means that data encoded with an older schema can be read with a newer schema
    * adding a field requires to specify a default value
  - Forward Compatibility means that data encoded with a newer schema can be read with an older schema
  - Full Compatibility means schemas are backward and forward compatible

---

# schema registry
https://docs.confluent.io/current/schema-registry/docs/index.html
http://cloudurable.com/blog/kafka-avro-schema-registry/index.html