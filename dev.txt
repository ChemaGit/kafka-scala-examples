---

> Data Serialization and Evolution

When sending data over the network or storing it in a file, we need a way to encode the data into bytes
* programming language specific serialization such as Java serialization makes consuming the data in other languages inconvenient
* language agnostic formats such as XML and JSON have two main drawbacks
  - allowing fields to be arbitrarily added or removed with lack of structure may cause data corruption
  - field names and type information are redundant, verbose and add overhead reducing performances
* alternatives are Avro, Thrift and Protocol Buffers
* Avro is recommended by Confluent, which provides enterprise support for Kafka ecosystem

---

> Avro Schema

An Avro schema defines the data structure in a JSON format

User.avsc
{
  "namespace": "com.kafka.demo",
  "type": "record",
  "name": "User",
  "fields": [
    {
      "name": "name",
      "type": "string"
    },
    {
      "name": "age",
      "type": "int"
    }
  ]
}

* can be used to serialize a Java object (POJO) into bytes and deserialize these bytes back into the Java object
* it requires a schema during both data serialization and deserialization
* because the schema is provided at decoding time, metadata such as the field names don't have to be explicitly encoded in the data
* provides schema evolution globally or for subjects
  - Backward Compatibility means that data encoded with an older schema can be read with a newer schema
    * adding a field requires to specify a default value
  - Forward Compatibility means that data encoded with a newer schema can be read with an older schema
  - Full Compatibility means schemas are backward and forward compatible

---

# TODO

# schema registry
https://docs.confluent.io/current/schema-registry/docs/index.html
https://www.sderosiaux.com/articles/2017/03/02/serializing-data-efficiently-with-apache-avro-and-dealing-with-a-schema-registry
https://web.archive.org/web/20180321112249/https://cloudurable.com/blog/kafka-avro-schema-registry/index.html
https://msayag.github.io/Kafka
http://bytepadding.com/big-data/spark/avro/avro-serialization-de-serialization-using-confluent-schema-registry
https://aseigneurin.github.io/2018/08/02/kafka-tutorial-4-avro-and-schema-registry.html

# bash import/export
https://github.com/confluentinc/schema-registry/issues/789

# more
https://registry-project.readthedocs.io/en/latest/
https://medium.com/@lukasgrasse/producing-and-consuming-avro-messages-over-kafka-in-scala-edc26d69298c

# example
https://github.com/confluentinc/examples/tree/5.0.0-post/clients/avro/src/main/java/io/confluent/examples/clients/basicavro
https://github.com/confluentinc/schema-registry/blob/master/client/src/main/java/io/confluent/kafka/schemaregistry/client/MockSchemaRegistryClient.java

# TODO

Schema ID allocation always happen in the master node and they ensure that the Schema IDs are monotonically increasing

Kafka is used as Schema Registry storage backend.
The special Kafka topic <kafkastore.topic> (default _schemas), with a single partition, is used as a highly available write ahead log.
All schemas, subject/version and ID metadata, and compatibility settings are appended as messages to this log.

Schema Registry is designed to work as a distributed service using single master architecture
Only the master is capable of publishing writes to the underlying Kafka log, but all nodes are capable of directly serving read requests.
Slave nodes serve registration requests indirectly by simply forwarding them to the current master, and returning the response supplied by the master.

---

https://github.com/cakesolutions/scala-kafka-client
https://github.com/lightbend/kafka-streams-scala
https://github.com/lightbend/kafka-with-akka-streams-kafka-streams-tutorial
https://github.com/Spinoco/fs2-kafka
